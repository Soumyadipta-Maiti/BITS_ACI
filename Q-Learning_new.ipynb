{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c5bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd208a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, num_states, num_actions, terminal_states, stochastic=False):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.terminal_states = terminal_states\n",
    "        self.stochastic = stochastic\n",
    "        self.transitions = np.zeros((num_states, num_actions, num_states), dtype=float)\n",
    "        self.rewards = np.zeros((num_states, num_actions), dtype=float)\n",
    "\n",
    "    def set_transitions(self, transition_matrix):\n",
    "        self.transitions = transition_matrix\n",
    "\n",
    "    def set_rewards(self, reward_matrix):\n",
    "        self.rewards = reward_matrix\n",
    "\n",
    "    def transition(self, state, action):\n",
    "        if self.stochastic:\n",
    "            probabilities = self.transitions[state, action, :]\n",
    "            next_state = np.random.choice(self.num_states, p=probabilities)\n",
    "        else:\n",
    "            next_state = self.transitions[state, action, :].argmax()\n",
    "\n",
    "        return next_state, self.rewards[state, action]\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=0.1, num_episodes=1000):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.num_episodes = num_episodes\n",
    "        self.q_table = np.zeros((self.env.num_states, self.env.num_actions), dtype=float)\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.num_episodes):\n",
    "            state = np.random.choice([s for s in range(self.env.num_states) if s not in self.env.terminal_states])\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                if np.random.rand() < self.epsilon:\n",
    "                    action = np.random.choice(range(self.env.num_actions))\n",
    "                else:\n",
    "                    action = np.argmax(self.q_table[state, :])\n",
    "\n",
    "                next_state, reward = self.env.transition(state, action)\n",
    "\n",
    "                q_next = np.max(self.q_table[next_state, :])\n",
    "                td_target = reward + self.gamma * q_next\n",
    "                td_error = td_target - self.q_table[state, action]\n",
    "\n",
    "                self.q_table[state, action] += self.alpha * td_error\n",
    "\n",
    "                if next_state in self.env.terminal_states:\n",
    "                    done = True\n",
    "                else:\n",
    "                    state = next_state\n",
    "\n",
    "            \n",
    "            print(f\"Episode {episode + 1} - Q-table:\")\n",
    "            print(self.q_table[::10, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02247e40",
   "metadata": {},
   "source": [
    "### Create the environment and Q-learning instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6a32bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 - Q-table:\n",
      "[[ 0.   0.   0.   0. ]\n",
      " [-0.1  0.   0.   0. ]\n",
      " [-0.1  0.   0.   0. ]\n",
      " [-0.1  0.   0.   0. ]\n",
      " [-0.1  0.   0.   0. ]\n",
      " [ 0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0. ]]\n",
      "Episode 2 - Q-table:\n",
      "[[ 0.      0.      0.      0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [-0.1     0.      0.      0.    ]\n",
      " [-0.1     0.     -0.1     0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "Episode 3 - Q-table:\n",
      "[[ 0.      0.      0.      0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [-0.1    -0.1099 -0.1     0.    ]\n",
      " [-0.1    -0.1099 -0.1     0.    ]\n",
      " [-0.1    -0.1099 -0.1     0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "Episode 4 - Q-table:\n",
      "[[ 0.      0.      0.      0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [-0.1    -0.1099 -0.1     0.    ]\n",
      " [-0.1    -0.1099 -0.1     0.    ]\n",
      " [-0.1    -0.1099 -0.1     0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "Episode 5 - Q-table:\n",
      "[[ 0.      0.      0.      0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [-0.1    -0.1099 -0.1     0.    ]\n",
      " [-0.1    -0.1099 -0.1     0.    ]\n",
      " [-0.1    -0.1099 -0.1     0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "Episode 6 - Q-table:\n",
      "[[ 0.      0.      0.      0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [-0.1    -0.1099 -0.1     0.    ]\n",
      " [-0.1    -0.1099 -0.1     0.    ]\n",
      " [-0.1    -0.1099 -0.1     0.    ]\n",
      " [-0.1    -0.1099  0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.    ]]\n",
      "Episode 7 - Q-table:\n",
      "[[ 0.         0.         0.         0.       ]\n",
      " [-0.1       -0.1099     0.         0.       ]\n",
      " [-0.1       -0.1099    -0.1099     0.       ]\n",
      " [-0.19      -0.2097901 -0.1099    -0.1999   ]\n",
      " [-0.19      -0.1099    -0.2997001 -0.1      ]\n",
      " [-0.2809    -0.20881   -0.29872   -0.1999   ]\n",
      " [-0.2809    -0.3157381 -0.2809    -0.2997001]\n",
      " [-0.2997001 -0.20881   -0.2809    -0.1999   ]\n",
      " [-0.2997001 -0.1999    -0.2809    -0.1999   ]\n",
      " [-0.2997001 -0.3077191 -0.2997001 -0.2997001]]\n",
      "Episode 8 - Q-table:\n",
      "[[ 0.          0.          0.          0.        ]\n",
      " [-0.19       -0.1099     -0.2187001  -0.1       ]\n",
      " [-0.271      -0.3157381  -0.30948328 -0.2997001 ]\n",
      " [-0.38238328 -0.30762109 -0.3077191  -0.2907901 ]\n",
      " [-0.38952019 -0.314758   -0.39930337 -0.2997001 ]\n",
      " [-0.46314919 -0.3077191  -0.29872    -0.2997001 ]\n",
      " [-0.2809     -0.3157381  -0.2809     -0.2997001 ]\n",
      " [-0.2997001  -0.20881    -0.2809     -0.1999    ]\n",
      " [-0.2997001  -0.1999     -0.2809     -0.1999    ]\n",
      " [-0.2997001  -0.3077191  -0.2997001  -0.2997001 ]]\n",
      "Episode 9 - Q-table:\n",
      "[[ 0.          0.          0.          0.        ]\n",
      " [-0.271      -0.21772    -0.2187001  -0.1999    ]\n",
      " [-0.3538     -0.3157381  -0.30948328 -0.2997001 ]\n",
      " [-0.47097395 -0.40476511 -0.4066175  -0.39049931]\n",
      " [-0.48842412 -0.41215775 -0.39930337 -0.3994004 ]\n",
      " [-0.46314919 -0.3077191  -0.3966571  -0.2997001 ]\n",
      " [-0.38238328 -0.41295251 -0.3806191  -0.49071843]\n",
      " [-0.3994004  -0.31671722 -0.38248031 -0.2997001 ]\n",
      " [-0.3994004  -0.2997001  -0.38248031 -0.2997001 ]\n",
      " [-0.38952019 -0.4066175  -0.3994004  -0.48992455]]\n",
      "Episode 10 - Q-table:\n",
      "[[ 0.          0.          0.          0.        ]\n",
      " [-0.271      -0.21772    -0.2187001  -0.1999    ]\n",
      " [-0.3538     -0.3157381  -0.30948328 -0.2997001 ]\n",
      " [-0.47097395 -0.40476511 -0.4066175  -0.39049931]\n",
      " [-0.48842412 -0.41215775 -0.39930337 -0.3994004 ]\n",
      " [-0.46314919 -0.3077191  -0.3966571  -0.2997001 ]\n",
      " [-0.38238328 -0.41295251 -0.3806191  -0.49071843]\n",
      " [-0.3994004  -0.31671722 -0.38248031 -0.2997001 ]\n",
      " [-0.3994004  -0.2997001  -0.38248031 -0.2997001 ]\n",
      " [-0.38952019 -0.4066175  -0.3994004  -0.48992455]]\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "num_states = 100\n",
    "num_actions = 4\n",
    "terminal_states = [0, 99]\n",
    "stochastic = False   # set True for stochastic case/multi state transition\n",
    "                        # False for deterministic case/single state transition\n",
    "\n",
    "env = Environment(num_states, num_actions, terminal_states, stochastic=stochastic)\n",
    "\n",
    "# Define the transition matrix and reward matrix\n",
    "transition_matrix = np.zeros((num_states, num_actions, num_states), dtype=float)\n",
    "reward_matrix = np.zeros((num_states, num_actions), dtype=float)\n",
    "\n",
    "for s in range(num_states):\n",
    "    for a in range(num_actions):\n",
    "        if s == 0:\n",
    "            transition_matrix[s, a, s] = 1.0\n",
    "        elif s == 99:\n",
    "            transition_matrix[s, a, s] = 1.0\n",
    "        else:\n",
    "            if a == 0: # Move up\n",
    "                if s >= 10:\n",
    "                    next_state = s - 10\n",
    "                else:\n",
    "                    next_state = s\n",
    "            elif a == 1: # Move right\n",
    "                if s % 10 != 9:\n",
    "                    next_state = s + 1\n",
    "                else:\n",
    "                    next_state = s\n",
    "            elif a == 2: # Move down\n",
    "                if s < 90:\n",
    "                    next_state = s + 10\n",
    "                else:\n",
    "                    next_state = s\n",
    "            elif a == 3: # Move left\n",
    "                if s % 10 != 0:\n",
    "                    next_state = s - 1\n",
    "                else:\n",
    "                    next_state = s\n",
    "\n",
    "            transition_matrix[s, a, next_state] = 1.0\n",
    "            reward_matrix[s, a] = -1.0\n",
    "\n",
    "env.set_transitions(transition_matrix)\n",
    "env.set_rewards(reward_matrix)\n",
    "\n",
    "# Create the Q-learning agent and train it\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "num_episodes = 10\n",
    "\n",
    "q_learning = QLearning(env, alpha=alpha, gamma=gamma, epsilon=epsilon, num_episodes=num_episodes)\n",
    "q_learning.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58129bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
