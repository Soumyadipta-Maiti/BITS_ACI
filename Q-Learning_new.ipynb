{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98fb3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca969dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, num_states, num_actions, terminal_states, stochastic=False):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.terminal_states = terminal_states\n",
    "        self.stochastic = stochastic\n",
    "        self.transitions = np.zeros((num_states, num_actions, num_states), dtype=float)\n",
    "        self.rewards = np.zeros((num_states, num_actions), dtype=float)\n",
    "\n",
    "    def set_transitions(self, transition_matrix):\n",
    "        self.transitions = transition_matrix\n",
    "\n",
    "    def set_rewards(self, reward_matrix):\n",
    "        self.rewards = reward_matrix\n",
    "\n",
    "    def transition(self, state, action):\n",
    "        if self.stochastic:\n",
    "            probabilities = self.transitions[state, action, :]\n",
    "            next_state = np.random.choice(self.num_states, p=probabilities)\n",
    "        else:\n",
    "            next_state = self.transitions[state, action, :].argmax()\n",
    "\n",
    "        return next_state, self.rewards[state, action]\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=0.1, num_episodes=1000):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.num_episodes = num_episodes\n",
    "        self.q_table = np.zeros((self.env.num_states, self.env.num_actions), dtype=float)\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.num_episodes):\n",
    "            state = np.random.choice([s for s in range(self.env.num_states) if s not in self.env.terminal_states])\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                if np.random.rand() < self.epsilon:\n",
    "                    action = np.random.choice(range(self.env.num_actions))\n",
    "                else:\n",
    "                    action = np.argmax(self.q_table[state, :])\n",
    "\n",
    "                next_state, reward = self.env.transition(state, action)\n",
    "\n",
    "                q_next = np.max(self.q_table[next_state, :])\n",
    "                td_target = reward + self.gamma * q_next\n",
    "                td_error = td_target - self.q_table[state, action]\n",
    "\n",
    "                self.q_table[state, action] += self.alpha * td_error\n",
    "\n",
    "                if next_state in self.env.terminal_states:\n",
    "                    done = True\n",
    "                else:\n",
    "                    state = next_state\n",
    "\n",
    "            \n",
    "            print(f\"Episode {episode + 1} - Q-table:\")\n",
    "            print(self.q_table[::10, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8add65",
   "metadata": {},
   "source": [
    "### Create the environment and Q-learning instances for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52c69790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 - Q-table:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Episode 2 - Q-table:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Episode 3 - Q-table:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Episode 4 - Q-table:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Episode 5 - Q-table:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Episode 6 - Q-table:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Episode 7 - Q-table:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Episode 8 - Q-table:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Episode 9 - Q-table:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Episode 10 - Q-table:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "num_states = 100\n",
    "num_actions = 4\n",
    "terminal_states = [0, 99]\n",
    "stochastic = False   # set True for stochastic case/multi state transition\n",
    "                        # False for deterministic case/single state transition\n",
    "\n",
    "env = Environment(num_states, num_actions, terminal_states, stochastic=stochastic)\n",
    "\n",
    "# Define the transition matrix and reward matrix\n",
    "transition_matrix = np.zeros((num_states, num_actions, num_states), dtype=float)\n",
    "reward_matrix = np.zeros((num_states, num_actions), dtype=float)\n",
    "\n",
    "for s in range(num_states):\n",
    "    for a in range(num_actions):\n",
    "        if s == 0:\n",
    "            transition_matrix[s, a, s] = 1.0\n",
    "        elif s == 99:\n",
    "            transition_matrix[s, a, s] = 1.0\n",
    "        else:\n",
    "            if a == 0: # Move up\n",
    "                if s >= 10:\n",
    "                    next_state = s - 10\n",
    "                else:\n",
    "                    next_state = s\n",
    "            elif a == 1: # Move right\n",
    "                if s % 10 != 9:\n",
    "                    next_state = s + 1\n",
    "                else:\n",
    "                    next_state = s\n",
    "            elif a == 2: # Move down\n",
    "                if s < 90:\n",
    "                    next_state = s + 10\n",
    "                else:\n",
    "                    next_state = s\n",
    "            elif a == 3: # Move left\n",
    "                if s % 10 != 0:\n",
    "                    next_state = s - 1\n",
    "                else:\n",
    "                    next_state = s\n",
    "\n",
    "            transition_matrix[s, a, next_state] = 1.0\n",
    "            reward_matrix[s, a] = -1.0\n",
    "\n",
    "env.set_transitions(transition_matrix)\n",
    "env.set_rewards(reward_matrix)\n",
    "\n",
    "# Create the Q-learning agent and train it\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "num_episodes = 10\n",
    "\n",
    "q_learning = QLearning(env, alpha=alpha, gamma=gamma, epsilon=epsilon, num_episodes=num_episodes)\n",
    "q_learning.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9a805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
